# Create pod security policy
apiVersion: extensions/v1beta1
kind: PodSecurityPolicy
metadata:
  name: psp-monitoring
spec:
  allowPrivilegeEscalation: false
  privileged: false
  runAsUser:
    rule: RunAsAny
  seLinux:
    rule: RunAsAny
  supplementalGroups:
    rule: RunAsAny
  fsGroup:
    rule: RunAsAny
  volumes:
    - 'configMap'
    - 'secret'
---
# Create namespace
apiVersion: v1
kind: Namespace
metadata:
  name: monitoring
---
# Create service account for Prometheus
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: monitoring
---
# Create default namespace service account
apiVersion: v1
kind: ServiceAccount
metadata:
  name: sa-monitoring
  namespace: monitoring
---
# Create defaulde namespace role
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: role-default-monitoring
  namespace: monitoring
rules:
- apiGroups:
  - extensions
  resourceNames:
  - psp-monitoring
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Attach default namespace role to default namespace service account
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: rolebinding-default-monitoring
  namespace: monitoring
roleRef:
  kind: Role
  name: role-default-monitoring
  apiGroup: rbac.authorization.k8s.io
subjects:
- kind: ServiceAccount
  name: sa-monitoring
---
# Create cluster role required for Prometheus
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - nodes/proxy
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups:
  - extensions
  resources:
  - ingresses
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
- apiGroups:
  - extensions
  resourceNames:
  - pks-privileged
  resources:
  - podsecuritypolicies
  verbs:
  - use
---
# Bind the service account to the aforementioned role
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: monitoring
---
# Create the configmap for Prometheus
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-server-conf
  labels:
    name: prometheus-server-conf
  namespace: monitoring
data:
  prometheus.yml: |-
    global:
      scrape_interval: 5s
      evaluation_interval: 5s
      
    alerting:
      alertmanagers:
        - static_configs:
          - targets: 
            - "alertmanager.monitoring.svc.cluster.local:80"

    rule_files:
      - /alertmanager/rules.yml    

    scrape_configs:
      - job_name: 'prometheus'
        static_configs: 
          - targets: ['localhost:9090']

      - job_name: 'kubernetes-apiservers'
        kubernetes_sd_configs:
        - role: endpoints
        scheme: https

        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
          insecure_skip_verify: true
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        relabel_configs:
        - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
          action: keep
          regex: default;kubernetes;https

      - job_name: 'kubernetes-nodes'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token

        kubernetes_sd_configs:
        - role: node

        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics

      - job_name: 'kubernetes-cadvisor'
        scheme: https
        tls_config:
          ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt
        bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token
        kubernetes_sd_configs:
        - role: node
        relabel_configs:
        - action: labelmap
          regex: __meta_kubernetes_node_label_(.+)
        - target_label: __address__
          replacement: kubernetes.default.svc:443
        - source_labels: [__meta_kubernetes_node_name]
          regex: (.+)
          target_label: __metrics_path__
          replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor
---
# Create Prometheus configmap for alerting rules
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-rules-conf
  labels:
    name: prometheus-rules-conf
  namespace: monitoring
data:
  rules.yml: |-
    groups:
      - name: targets
        rules:
          - alert: monitor_service_down
            expr: up == 0
            for: 30s
            labels:
              severity: critical
            annotations:
              summary: "Monitor service non-operational"
              description: "Service {{ $labels.instance }} is down."
          - alert: high_cpu_load
            expr: node_load1 > 1.5
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "Server under high load"
              description: "Docker host is under high load, the avg load 1m is at {{ $value}}. Reported by instance {{ $labels.instance }} of job {{ $labels.job }}."
          - alert: high_memory_load
            expr: (sum(node_memory_MemTotal_bytes) - sum(node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes) ) / sum(node_memory_MemTotal_bytes) * 100 > 85
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "Server memory is almost full"
              description: "Docker host memory usage is {{ humanize $value}}%. Reported by instance {{ $labels.instance }} of job {{ $labels.job }}."
          - alert: high_storage_load
            expr: (node_filesystem_size_bytes{fstype="aufs"} - node_filesystem_free_bytes{fstype="aufs"}) / node_filesystem_size_bytes{fstype="aufs"}  * 100 > 85
            for: 30s
            labels:
              severity: warning
            annotations:
              summary: "Server storage is almost full"
              description: "Docker host storage usage is {{ humanize $value}}%. Reported by instance {{ $labels.instance }} of job {{ $labels.job }}."
---
# Create deployment for Prometheus
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: prometheus-deployment
  namespace: monitoring
spec:
  replicas: 1
  template:
    metadata:
      labels:
        app: prometheus-server
    spec:
      serviceAccountName: prometheus
      containers:
        - name: prometheus
          image: prom/prometheus:v2.9.2
          args:
            - "--config.file=/etc/prometheus/prometheus.yml"
            - "--storage.tsdb.path=/prometheus/"
          ports:
            - containerPort: 9090
          volumeMounts:
            - name: prometheus-config-volume
              mountPath: /etc/prometheus/
            - name: prometheus-storage-volume
              mountPath: /prometheus/
            - name: prometheus-rules-volume
              mountPath: /alertmanager/
      volumes:
        - name: prometheus-config-volume
          configMap:
            defaultMode: 420
            name: prometheus-server-conf
            
        - name: prometheus-rules-volume
          configMap:
            name: prometheus-rules-conf
  
        - name: prometheus-storage-volume
          emptyDir: {}
---
# Expose Prometheus via loadbalancer
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  annotations:
      prometheus.io/scrape: 'true'
      prometheus.io/path:   /
      prometheus.io/port:   '9090'
  namespace: monitoring  
spec:
  selector: 
    app: prometheus-server
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 9090 
---
# Install Grafana
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana-deployment
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:6.1.6
        ports:
        - containerPort: 80
      serviceAccountName: sa-monitoring
---
# Expose Grafana via loadbalancer
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  selector: 
    app: grafana
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 3000 
---
# Install Alertmanager
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager-deployment
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.17.0
        ports:
        - containerPort: 9093
      serviceAccountName: sa-monitoring
---
# Expose Alertmanager via Loadbalancer
apiVersion: v1
kind: Service
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  selector: 
    app: alertmanager
  type: LoadBalancer
  ports:
    - port: 80
      targetPort: 9093 
